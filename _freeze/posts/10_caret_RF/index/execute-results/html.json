{
  "hash": "c0aaac803bb360b042cc354681457b5f",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Random Forest with Caret\"\nauthor: \"Jie Wu\"\ndraft: false\ndate: \"2025-06-20\"\nformat: \n    html:\n        toc: true\n        toc-depth: 2\n        code-fold: false\ncategories: [\"Machine Learning\", \"Data Science\", \"R Programming\"]\n---\n\nIn this post, we will use the `caret` package to implement a more standardized machine learning workflow. \n\nThe `caret` (Classification And REgression Training) package in R is a comprehensive toolkit for building, evaluating, and tuning machine learning models. It provides a unified interface to hundreds of algorithms, streamlines data preprocessing, supports resampling methods, and simplifies model comparison. By standardizing workflows, `caret` makes it easier to develop robust and reproducible predictive models in R. \n\nLet's load the prepare the data table again:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load all packages\nrequire(tidyverse)\nrequire(here)\nrequire(caret) \n\n# Load the mutation table from the RDS file saved in the last post\ndata <- readRDS(here(\"_data/combined_data.rds\"))\n\nfile_path <- here(\"_data/cancerGeneList.tsv\")\n\nGene_anno <- read_tsv(file_path, guess_max = Inf)\n\n# pull genes in MSK-IMPACT panel \n\nMSK_genes <- Gene_anno %>% filter(`MSK-IMPACT` == \"Yes\") %>%\n            pull(`Hugo Symbol`) %>% unique()\n\ndata_rf <- data %>% \n        dplyr::select(any_of(paste0(\"mut.\", MSK_genes)),\n                                    WGD) %>%\n                filter(! is.na(WGD))\n\n# remove \"-\" from names to avoid errors running RF               \nnames(data_rf) <- gsub(\"-\", \"\", names(data_rf))\n\ndata_rf <- data_rf %>% mutate(WGD = as.factor(WGD))\n```\n:::\n\n\n`caret` has handy functions to create training and test sets easily. We can use the `createDataPartition()` function to split our data into training and testing sets while preserving the class distribution of the target variable. We can then use `trainControl()` to set up cross-validation parameteres. \n\nHere's how we can do it: (The training will take a while)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(23)\ntrain_idx <- createDataPartition(data_rf$WGD, p = 0.8, list = FALSE)\ntrain_data <- data_rf[train_idx, ]\ntest_data <- data_rf[-train_idx, ]\n\n# Set up cross-validation\nctrl <- trainControl(method = \"cv\", number = 10)\n\nrf_model <- train(\n    WGD ~ .,\n    data = train_data,\n    method = \"rf\",\n    trControl = ctrl,\n    importance = TRUE\n)\n\n\nprint(rf_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRandom Forest \n\n1286 samples\n 475 predictor\n   2 classes: '0', '1' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 1158, 1157, 1157, 1158, 1157, 1157, ... \nResampling results across tuning parameters:\n\n  mtry  Accuracy   Kappa    \n    2   0.6609738  0.0000000\n  238   0.6570494  0.2494827\n  475   0.6500484  0.2411229\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was mtry = 2.\n```\n\n\n:::\n\n```{.r .cell-code}\nrandomForest::varImpPlot(rf_model$finalModel, n.var=10)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Predict on test set\nrf_pred <- predict(rf_model, newdata = test_data)\n\nconfusionMatrix(rf_pred, test_data$WGD)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0   0   0\n         1 109 212\n                                          \n               Accuracy : 0.6604          \n                 95% CI : (0.6058, 0.7121)\n    No Information Rate : 0.6604          \n    P-Value [Acc > NIR] : 0.526           \n                                          \n                  Kappa : 0               \n                                          \n Mcnemar's Test P-Value : <2e-16          \n                                          \n            Sensitivity : 0.0000          \n            Specificity : 1.0000          \n         Pos Pred Value :    NaN          \n         Neg Pred Value : 0.6604          \n             Prevalence : 0.3396          \n         Detection Rate : 0.0000          \n   Detection Prevalence : 0.0000          \n      Balanced Accuracy : 0.5000          \n                                          \n       'Positive' Class : 0               \n                                          \n```\n\n\n:::\n:::\n\n\nLet's take a look at top features, we use `ComplexHeatmap`, which is my favorite heatmap package. We only use a subset of the cell lines for better visualiztion.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntopfeatures <- \n    randomForest::importance(rf_model$finalModel)  %>% \n    as.data.frame() %>% \n    arrange(-`0`) %>% \n    head(10)  %>% \n    row.names()\n\nrequire(ComplexHeatmap)\n\n\nHeatmap(\n    data_rf %>% arrange(WGD) %>% dplyr::select(topfeatures),\n    left_annotation = HeatmapAnnotation(\n        df = (data_rf %>% arrange(WGD) %>% dplyr::select(WGD)),\n        which = \"row\"\n    ),\n    cluster_rows = FALSE,\n    border = FALSE,\n    col = c(\"0\" = \"white\", \"1\" = \"red\")\n)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}