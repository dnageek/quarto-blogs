---
title: "Tidyverse: Read Data"
author: "Jie Wu"
draft: false
date: "2025-05-09T19:45:00"
format: 
    html:
        toc: true
        toc-depth: 2
        code-fold: false
categories: ["Tidyverse", "Data Science", "R Programming"]
---

# Functions to read different kinds of data in Tidyverse

Below is a summary of Tidyverse packages and their functions for importing data (by ChatGPT). Use this as a quick reference, and consult the official documentation or search online for detailed usage and options. You don't need to memorize everything—just know where to look when needed.

| **Package** | **Function** | **Purpose** | **File Type / Input** |
|------------------|------------------|------------------|------------------|
| **readr** | `read_csv()` | Read comma-separated values | `.csv` |
|  | `read_tsv()` | Read tab-separated values | `.tsv` |
|  | `read_delim()` | Read delimited text files | Custom delimiter |
|  | `read_fwf()` | Read fixed-width text files | `.txt`, structured text |
|  | `read_table()` | Read whitespace-separated files | `.txt`, space-separated |
|  | `read_lines()` | Read lines as character vector | Raw text |
|  | `read_file()` | Read entire file as one string | Raw text |
|  | `read_rds()` / `write_rds()` | Read/write R serialized objects | `.rds` |
|  | `type_convert()` | Guess and convert column types | Tibble/data frame |
| **readxl** | `read_excel()` | Read Excel files | `.xls`, `.xlsx` |
| **haven** | `read_spss()` | Import SPSS data | `.sav`, `.zsav` |
|  | `read_stata()` | Import Stata data | `.dta` |
|  | `read_sas()` | Import SAS data | `.sas7bdat` |
| **xml2** | `read_xml()` | Parse XML | `.xml`, XML string/URL |
|  | `read_html()` | Parse HTML | `.html`, HTML string/URL |
|  | `xml_find_all()` | Find all nodes via XPath | XPath expression |
|  | `xml_find_first()` | Find first matching node | XPath expression |
|  | `xml_text()` | Extract text content | XML/HTML node |
|  | `xml_attr()` / `xml_attrs()` | Extract attribute(s) | XML/HTML node |
|  | `xml_children()` | Get child nodes | XML/HTML node |
|  | `xml_structure()` | Display tree structure | XML/HTML document |
| **httr** | `GET()` / `POST()` | Make HTTP requests | API endpoints, web URLs |
|  | `content()` | Extract content from response | Parsed text, JSON, XML, raw |
|  | `status_code()` | Check HTTP response code | HTTP response object |
|  | `add_headers()` | Add custom headers to request | Authorization, content-type, etc. |
|  | `authenticate()` | Add basic authentication credentials | Username/password |
| **rvest** | `read_html()` | (Wraps `xml2::read_html()`) | Web page scraping |
|  | `html_nodes()` | Select multiple HTML elements (CSS/XPath) | CSS or XPath selector |
|  | `html_node()` | Select a single HTML element | CSS or XPath selector |
|  | `html_text()` | Extract visible text | HTML node |
|  | `html_attr()` | Extract attribute value | HTML tag attribute (e.g., `href`, `src`) |
|  | `html_table()` | Extract HTML tables into tibbles | `<table>` elements |

# Read DepMap Mutation Table

Before proceeding, ensure you have the `tidyverse` package installed by running `install.packages("tidyverse")`.

We will work with the `OmicsSomaticMutations.csv` file from the 24Q4 DepMap release. The file can be downloaded from the following link: <https://plus.figshare.com/ndownloader/files/51065732>. While `readr` functions can read files directly from a URL, we will download the file locally since it will be used multiple times.

For the first attempt, we will use `read_csv()` with its default parameters.

```{r}
#| message: false
#| warning: false
# Load all packages
require(readr)
require(dplyr)
require(here)

url <- "https://plus.figshare.com/ndownloader/files/51065732"
file_path <- here("_data/OmicsSomaticMutations.csv")
download.file(url, file_path )

depmap_mutation_data <- read_csv(file_path)

```

Of course, the code runs and loads all rows and columns, but did you notice those warning messages? Don't ignore them—they often provide valuable insights into potential issues with your data!

![](images/paste-2.png)

Let's run the suggested `problems()` command as suggested!

```{r}
problems(depmap_mutation_data)

```

If you examine the first problem as an example, you'll notice that the 60th column (`GwasDisease`) is empty until row 12,396. To investigate further, I ran the following command to extract all values in this column and display the unique values as a quick spot-check:

```{r}
depmap_mutation_data %>% pull(GwasDisease) %>% unique()
```

Did you notice? The entire column is filled with `NA` values, indicating that it was not properly loaded using the default `read_csv()` function. This highlights the importance of carefully reviewing your data after importing it. Always double-check for issues like this to ensure your analysis starts on the right foot.

While `read_csv()` is generally a better option than `read.table()`—offering convenience by automatically handling quotes, separators, and headers—it is not foolproof. Even if your data is truly a CSV file, the importer may still encounter issues and fail to load your data correctly. This is a critical point to remember for any data scientist or bioinformatician: always validate your imported data to ensure accuracy and reliability.

```{r}
depmap_mutation_data <- read_csv(file_path, guess_max = Inf)

# Check for problems again
problems(depmap_mutation_data)

# Verify the column values
depmap_mutation_data %>% pull(GwasDisease) %>% unique()
```

As you can see, no problems were found, and the column now contains more values.

Even if there are no warnings now, it's always a good practice to perform a few quick checks to inspect your data. You don't want to encounter issues later due to unnoticed problems in the initial step. Some example commands you can use include `dim()`, `head()`, `spec()` and `names()`. 